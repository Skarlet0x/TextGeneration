{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKqFu3gG9tKJ"
      },
      "source": [
        "##IMPORTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JVPgrNNPhW02"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VbUaw8ubhmh-"
      },
      "outputs": [],
      "source": [
        "dataset = open(\"corpus.txt\", \"r\", encoding = \"utf-8-sig\").read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UMXoAzbCmWat"
      },
      "outputs": [],
      "source": [
        "dataset[:100]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIa1B6h5iOn-"
      },
      "source": [
        "##PREPROCESSING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MCi70KwYl1Q0"
      },
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "    # lower-case\n",
        "    text = text.lower()\n",
        "\n",
        "    # clean archaisms and other special cases\n",
        "    text = re.sub(r\"'cause \", \"because\", text)\n",
        "    text = re.sub(r\"shalt\", \"shall\", text)\n",
        "    text = re.sub(r\"kaos\", \"chaos\", text)\n",
        "    text = re.sub(r\"'till\", \"until\", text)\n",
        "    text = re.sub(r\"fuckin'\", \"fucking\", text)\n",
        "    text = re.sub(r\"couldst\", \"could\", text)\n",
        "    text = re.sub(r\"sayeth\", \"says\", text)\n",
        "    text = re.sub(r\"calleth\", \"calls\", text)\n",
        "    text = re.sub(r\"&\", \" and \", text)\n",
        "    text = re.sub(r\"sathan\", \"satan\", text)\n",
        "    text = re.sub(r\"'em\", \"them\", text)\n",
        "\n",
        "    # possible parts of other words\n",
        "    text = re.sub(r\"(?<![a-zA-Z])thy(?![a-zA-Z])|thine\", \"your\", text)\n",
        "    text = re.sub(r\"thou|thee|(?<![a-zA-Z])ye(?![a-zA-Z])\", \"you\", text)\n",
        "    text = re.sub(r\"(?<![a-zA-Z])o(?![a-zA-Z])\", \"oh\", text)\n",
        "    text = re.sub(r\"(?<![a-zA-Z])'tis(?![a-zA-Z])\", \"it is\", text)\n",
        "    text = re.sub(r\"(?<![a-zA-Z])thru(?![a-zA-Z])\", \"through\", text)\n",
        "    text = re.sub(r\"(?<![a-zA-Z])ov(?![a-zA-Z])\", \"of\", text)\n",
        "    text = re.sub(r\"(?<![a-zA-Z])hast(?![a-zA-Z])\", \"have\", text)\n",
        "\n",
        "    # punctuation, except \\n and '\n",
        "    text = re.sub(r\"[^\\na-zA-Z']\", \" \", text)\n",
        "\n",
        "    # double whitespaces\n",
        "    text = re.sub(r\" +\", \" \", text)\n",
        "\n",
        "\n",
        "  \n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lPITHVWqyLr9"
      },
      "outputs": [],
      "source": [
        "text = clean_text(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CffHVigEyPqM"
      },
      "outputs": [],
      "source": [
        "tokens = re.findall(r\"\\S+|\\n\", text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nqTSI9-VmlKL"
      },
      "outputs": [],
      "source": [
        "tokens[600:666]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5YAehfPnJzD"
      },
      "source": [
        "##EXPLORATORY DATA ANALYSIS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eAJ1yvjQnORL"
      },
      "outputs": [],
      "source": [
        "# total tokens\n",
        "len(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RaEZRzvanRWb"
      },
      "outputs": [],
      "source": [
        "# unique tokens\n",
        "len(set(tokens))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z-2FLMhZtLeR"
      },
      "outputs": [],
      "source": [
        "# hapax legomena - outliers - possibly remove?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xN1s12ZEe1dL"
      },
      "outputs": [],
      "source": [
        "# vocab analysis - lemmatize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-IJfw8xj7T9"
      },
      "source": [
        "##SEQUENCES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vigKecggiMiG"
      },
      "outputs": [],
      "source": [
        "seq_len = 50\n",
        "X_items = []\n",
        "y = []\n",
        "\n",
        "for i in range(0, len(tokens)-seq_len):\n",
        "\tX_items.append(tokens[i:i+seq_len])\n",
        "\ty.append(tokens[i+seq_len])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYrD5rpil1wu"
      },
      "outputs": [],
      "source": [
        "X = [\" \".join(item) for item in X_items]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jf-LYb1SOKka"
      },
      "outputs": [],
      "source": [
        "X[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vrw4WO1JvFts"
      },
      "outputs": [],
      "source": [
        "y[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I0NslwfFk9gx"
      },
      "outputs": [],
      "source": [
        "# TESTING THE DIMENSIONS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xtcp00C2heYT"
      },
      "outputs": [],
      "source": [
        "x_test = [re.findall(r\"\\S+|\\n\", sequence) for sequence in X]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j8_uutHDhkjj"
      },
      "outputs": [],
      "source": [
        "lengths_x = [len(item) for item in x_test]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3FFulpKchsdL"
      },
      "outputs": [],
      "source": [
        "set(lengths_x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zwOF7VFoxhRn"
      },
      "outputs": [],
      "source": [
        "y_test = [re.findall(r\"\\S+|\\n\", sequence) for sequence in y]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xcNX0NgXxj83"
      },
      "outputs": [],
      "source": [
        "lengths_y = [len(item) for item in y_test]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gh8LHkSLxnaA"
      },
      "outputs": [],
      "source": [
        "set(lengths_y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C73Ou5QQlBlR"
      },
      "source": [
        "##TOKENIZATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eo3T0ETW9OPI"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# to keep the \\n\n",
        "filters_ = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t' \n",
        "tokenizer = Tokenizer(filters=filters_)\n",
        "\n",
        "tokenizer.fit_on_texts(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0GoEBiWxrSkI"
      },
      "outputs": [],
      "source": [
        "X_emb = tokenizer.texts_to_sequences(X)\n",
        "y_emb = tokenizer.texts_to_sequences(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpa3yztMzKdV"
      },
      "outputs": [],
      "source": [
        "# TESTING THE DIMENSIONS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BAiPdSwMyo6u"
      },
      "outputs": [],
      "source": [
        "lengths_X_emb = [len(item) for item in X_emb]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w-bBmzR7yuKG"
      },
      "outputs": [],
      "source": [
        "set(lengths_X_emb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68TQhKUmyl12"
      },
      "outputs": [],
      "source": [
        "lengths_y_emb = [len(item) for item in y_emb]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K43cpTivyxvO"
      },
      "outputs": [],
      "source": [
        "set(lengths_y_emb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sTBTbCOTt7m2"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "classes = len(tokenizer.word_index)\n",
        "\n",
        "y_categorical = to_categorical(y_emb, num_classes=classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGa5IgHNiSLt"
      },
      "source": [
        "##MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y4ca6khquEjy"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Input, Embedding, LSTM, Dense, Dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-DGk5w32uO0g"
      },
      "outputs": [],
      "source": [
        "INPUT_SHAPE = seq_len\n",
        "VOCABULARY_SIZE = len(tokenizer.word_index)\n",
        "EMBEDDING_DIMENSION = 50\n",
        "LSTM_UNITS = 50\n",
        "DROPOUT_RATE = 0.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "571HSDYJeods"
      },
      "outputs": [],
      "source": [
        "rnn = Sequential()\n",
        "\n",
        "rnn.add(Input(INPUT_SHAPE,))\n",
        "# +1 here resolves the indexing problem during training\n",
        "rnn.add(Embedding(VOCABULARY_SIZE+1, EMBEDDING_DIMENSION))\n",
        "rnn.add(LSTM(LSTM_UNITS, return_sequences=True))\n",
        "rnn.add(Dense(VOCABULARY_SIZE, activation=\"softmax\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GilsCwQceosi"
      },
      "outputs": [],
      "source": [
        "rnn.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ClIhRRUvvWSB"
      },
      "outputs": [],
      "source": [
        "rnn.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VESfj6AVjYus"
      },
      "source": [
        "##MODEL TRAINING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jFAN8ATE258S"
      },
      "outputs": [],
      "source": [
        "# fixes some errors\n",
        "X_train = np.array(X_emb, dtype=float)\n",
        "y_train = np.array(y_categorical, dtype=float)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4_2cPaIJ258S"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 32\n",
        "EPOCHS = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TzcfbFB1vW4Y"
      },
      "outputs": [],
      "source": [
        "rnn.fit(X_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvEXsjvu-HVb"
      },
      "source": [
        "##GENERATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FQwsMcT1epCK"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "TextGeneration.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}